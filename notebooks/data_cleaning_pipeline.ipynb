{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03040c55",
   "metadata": {},
   "source": [
    "# Data Cleaning Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d6a40a",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032df2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "Pandas version: 2.2.3\n",
      "NumPy version: 2.2.6\n",
      "Matplotlib version: 3.10.0\n",
      "Seaborn version: 0.13.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "from typing import Tuple, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 2)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e44f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Output directory configured: ../outputs/images/cleaning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "OUTPUT_DIR = '../outputs/images/cleaning'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def save_figure(filename, dpi=300, bbox_inches='tight'):\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    plt.savefig(filepath, dpi=dpi, bbox_inches=bbox_inches)\n",
    "    print(f\"Saved figure: {filename}\")\n",
    "\n",
    "print(f\"Output directory configured: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7aee53",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Type Enforcement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ae5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_enforce_types(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load dataset and enforce correct data types.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 1: DATA LOADING AND TYPE ENFORCEMENT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    dtype_spec = {\n",
    "        'order_category': 'category',\n",
    "        'item_count': 'Int64',\n",
    "        'actual_delivery_time_minutes': 'float64',\n",
    "        'estimated_delivery_time_lower_minutes': 'float64',\n",
    "        'estimated_delivery_time_upper_minutes': 'float64',\n",
    "        'venue_location_h3_index': 'str',\n",
    "        'customer_location_h3_index': 'str',\n",
    "        'courier_supply_index': 'float64',\n",
    "        'precipitation': 'float64'\n",
    "    }\n",
    "    \n",
    "    df = pd.read_csv(filepath, dtype=dtype_spec, parse_dates=['order_placed_at_utc'])\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    print(f\"\\nLoaded {initial_rows:,} rows from {filepath}\")\n",
    "    \n",
    "    if df['order_placed_at_utc'].dt.tz is None:\n",
    "        df['order_placed_at_utc'] = df['order_placed_at_utc'].dt.tz_localize('UTC')\n",
    "        print(\"Localized timestamps to UTC\")\n",
    "    else:\n",
    "        df['order_placed_at_utc'] = df['order_placed_at_utc'].dt.tz_convert('UTC')\n",
    "        print(\"Converted timestamps to UTC\")\n",
    "    \n",
    "    assert df['order_placed_at_utc'].dtype == 'datetime64[ns, UTC]'\n",
    "    assert df['order_category'].dtype.name == 'category'\n",
    "    assert df['venue_location_h3_index'].dtype == 'object'\n",
    "    assert df['customer_location_h3_index'].dtype == 'object'\n",
    "    \n",
    "    print(\"\\nType enforcement complete\")\n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(f\"\\nDate range: {df['order_placed_at_utc'].min()} to {df['order_placed_at_utc'].max()}\")\n",
    "    print(f\"Number of days: {(df['order_placed_at_utc'].max() - df['order_placed_at_utc'].min()).days}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4978238d",
   "metadata": {},
   "source": [
    "## 3. Structural Validation (Duplicates)\n",
    "\n",
    "Remove exact duplicate rows and identify potential duplicate orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
    "    \"\"\"Remove exact and potential duplicate orders.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 2: STRUCTURAL VALIDATION (DUPLICATES)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    print(f\"\\nStarting rows: {initial_rows:,}\")\n",
    "    \n",
    "    df_clean = df.drop_duplicates()\n",
    "    exact_duplicates_removed = initial_rows - len(df_clean)\n",
    "    print(f\"\\nRemoved {exact_duplicates_removed:,} exact duplicate rows\")\n",
    "    \n",
    "    duplicate_cols = [\n",
    "        'order_placed_at_utc',\n",
    "        'venue_location_h3_index',\n",
    "        'customer_location_h3_index',\n",
    "        'item_count',\n",
    "        'estimated_delivery_time_lower_minutes',\n",
    "        'estimated_delivery_time_upper_minutes'\n",
    "    ]\n",
    "    \n",
    "    before_potential = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates(subset=duplicate_cols, keep='first')\n",
    "    potential_duplicates_removed = before_potential - len(df_clean)\n",
    "    print(f\"Removed {potential_duplicates_removed:,} potential duplicate orders\")\n",
    "    \n",
    "    total_removed = initial_rows - len(df_clean)\n",
    "    print(f\"\\nTotal rows removed: {total_removed:,}\")\n",
    "    print(f\"Remaining rows: {len(df_clean):,}\")\n",
    "    \n",
    "    log = {\n",
    "        'exact_duplicates': exact_duplicates_removed,\n",
    "        'potential_duplicates': potential_duplicates_removed\n",
    "    }\n",
    "    \n",
    "    return df_clean, log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6934fe5a",
   "metadata": {},
   "source": [
    "## 4. Target Cleaning\n",
    "\n",
    "Clean the target variable `actual_delivery_time_minutes`:\n",
    "- Remove non-positive values\n",
    "- Remove unrealistic values (> 180 minutes = 3 hours)\n",
    "- Do not impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892ecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_target(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
    "    \"\"\"Clean target variable by removing invalid values.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3: TARGET CLEANING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    print(f\"\\nStarting rows: {initial_rows:,}\")\n",
    "    \n",
    "    print(\"\\nTarget statistics BEFORE cleaning:\")\n",
    "    print(df['actual_delivery_time_minutes'].describe())\n",
    "    print(f\"Missing values: {df['actual_delivery_time_minutes'].isna().sum()}\")\n",
    "    \n",
    "    non_positive = (df['actual_delivery_time_minutes'] <= 0).sum()\n",
    "    df_clean = df[df['actual_delivery_time_minutes'] > 0].copy()\n",
    "    print(f\"\\nRemoved {non_positive:,} rows with actual_delivery_time_minutes <= 0\")\n",
    "    \n",
    "    unrealistic = (df_clean['actual_delivery_time_minutes'] > 180).sum()\n",
    "    df_clean = df_clean[df_clean['actual_delivery_time_minutes'] <= 180].copy()\n",
    "    print(f\"Removed {unrealistic:,} rows with actual_delivery_time_minutes > 180\")\n",
    "    \n",
    "    print(\"\\nTarget statistics AFTER cleaning:\")\n",
    "    print(df_clean['actual_delivery_time_minutes'].describe())\n",
    "    \n",
    "    total_removed = initial_rows - len(df_clean)\n",
    "    print(f\"\\nTotal rows removed: {total_removed:,}\")\n",
    "    print(f\"Remaining rows: {len(df_clean):,}\")\n",
    "    \n",
    "    log = {\n",
    "        'target_non_positive': non_positive,\n",
    "        'target_unrealistic': unrealistic\n",
    "    }\n",
    "    \n",
    "    return df_clean, log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d2de8",
   "metadata": {},
   "source": [
    "## 5. ETA Consistency Checks\n",
    "\n",
    "Validate estimated delivery time bounds and create derived features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8114a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_engineer_eta(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
    "    \"\"\"Validate ETA consistency and create derived features.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 4: ETA CONSISTENCY CHECKS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    print(f\"\\nStarting rows: {initial_rows:,}\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    lower_invalid = (df_clean['estimated_delivery_time_lower_minutes'] <= 0).sum()\n",
    "    df_clean = df_clean[df_clean['estimated_delivery_time_lower_minutes'] > 0].copy()\n",
    "    print(f\"\\nRemoved {lower_invalid:,} rows with estimated_delivery_time_lower_minutes <= 0\")\n",
    "    \n",
    "    upper_invalid = (df_clean['estimated_delivery_time_upper_minutes'] <= 0).sum()\n",
    "    df_clean = df_clean[df_clean['estimated_delivery_time_upper_minutes'] > 0].copy()\n",
    "    print(f\"Removed {upper_invalid:,} rows with estimated_delivery_time_upper_minutes <= 0\")\n",
    "    \n",
    "    inconsistent = (\n",
    "        df_clean['estimated_delivery_time_lower_minutes'] > \n",
    "        df_clean['estimated_delivery_time_upper_minutes']\n",
    "    ).sum()\n",
    "    df_clean = df_clean[\n",
    "        df_clean['estimated_delivery_time_lower_minutes'] <= \n",
    "        df_clean['estimated_delivery_time_upper_minutes']\n",
    "    ].copy()\n",
    "    print(f\"Removed {inconsistent:,} rows where lower > upper\")\n",
    "    \n",
    "    df_clean['estimate_width'] = (\n",
    "        df_clean['estimated_delivery_time_upper_minutes'] - \n",
    "        df_clean['estimated_delivery_time_lower_minutes']\n",
    "    )\n",
    "    df_clean['estimate_midpoint'] = (\n",
    "        df_clean['estimated_delivery_time_upper_minutes'] + \n",
    "        df_clean['estimated_delivery_time_lower_minutes']\n",
    "    ) / 2\n",
    "    print(\"\\nCreated derived features: estimate_width, estimate_midpoint\")\n",
    "    \n",
    "    print(\"\\nEstimate width statistics:\")\n",
    "    print(df_clean['estimate_width'].describe())\n",
    "    \n",
    "    zero_width = (df_clean['estimate_width'] == 0).sum()\n",
    "    large_width = (df_clean['estimate_width'] > 180).sum()\n",
    "    print(f\"\\nRows with zero estimate width: {zero_width:,} ({zero_width/len(df_clean)*100:.2f}%)\")\n",
    "    print(f\"Rows with estimate width > 180 minutes: {large_width:,} ({large_width/len(df_clean)*100:.2f}%)\")\n",
    "    \n",
    "    total_removed = initial_rows - len(df_clean)\n",
    "    print(f\"\\nTotal rows removed: {total_removed:,}\")\n",
    "    print(f\"Remaining rows: {len(df_clean):,}\")\n",
    "    \n",
    "    log = {\n",
    "        'eta_lower_invalid': lower_invalid,\n",
    "        'eta_upper_invalid': upper_invalid,\n",
    "        'eta_inconsistent': inconsistent,\n",
    "        'zero_width_count': zero_width,\n",
    "        'large_width_count': large_width\n",
    "    }\n",
    "    \n",
    "    return df_clean, log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d4d6ab",
   "metadata": {},
   "source": [
    "## 6. Feature Sanity Checks\n",
    "\n",
    "Validate and clean individual features based on domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
    "    \"\"\"Validate and clean individual features based on domain knowledge.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 5: FEATURE SANITY CHECKS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    print(f\"\\nStarting rows: {initial_rows:,}\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    print(\"\\n--- courier_supply_index ---\")\n",
    "    print(f\"Range: [{df_clean['courier_supply_index'].min():.2f}, {df_clean['courier_supply_index'].max():.2f}]\")\n",
    "    courier_invalid = (df_clean['courier_supply_index'] < 0).sum()\n",
    "    df_clean = df_clean[df_clean['courier_supply_index'] >= 0].copy()\n",
    "    print(f\"Removed {courier_invalid:,} rows with courier_supply_index < 0\")\n",
    "    \n",
    "    print(\"\\n--- precipitation ---\")\n",
    "    print(f\"Range: [{df_clean['precipitation'].min():.2f}, {df_clean['precipitation'].max():.2f}]\")\n",
    "    precip_negative = (df_clean['precipitation'] < 0).sum()\n",
    "    df_clean = df_clean[df_clean['precipitation'] >= 0].copy()\n",
    "    print(f\"Removed {precip_negative:,} rows with precipitation < 0\")\n",
    "    \n",
    "    precip_excessive = (df_clean['precipitation'] > 100).sum()\n",
    "    df_clean = df_clean[df_clean['precipitation'] <= 100].copy()\n",
    "    print(f\"Removed {precip_excessive:,} rows with precipitation > 100\")\n",
    "    \n",
    "    print(\"\\n--- item_count ---\")\n",
    "    print(f\"Range: [{df_clean['item_count'].min()}, {df_clean['item_count'].max()}]\")\n",
    "    item_invalid = (df_clean['item_count'] <= 0).sum()\n",
    "    df_clean = df_clean[df_clean['item_count'] > 0].copy()\n",
    "    print(f\"Removed {item_invalid:,} rows with item_count <= 0\")\n",
    "    \n",
    "    print(\"\\nNumeric feature summary:\")\n",
    "    numeric_cols = [\n",
    "        'courier_supply_index', \n",
    "        'precipitation', \n",
    "        'item_count',\n",
    "        'actual_delivery_time_minutes'\n",
    "    ]\n",
    "    print(df_clean[numeric_cols].describe())\n",
    "    \n",
    "    total_removed = initial_rows - len(df_clean)\n",
    "    print(f\"\\nTotal rows removed: {total_removed:,}\")\n",
    "    print(f\"Remaining rows: {len(df_clean):,}\")\n",
    "    \n",
    "    log = {\n",
    "        'courier_supply_invalid': courier_invalid,\n",
    "        'precipitation_negative': precip_negative,\n",
    "        'precipitation_excessive': precip_excessive,\n",
    "        'item_count_invalid': item_invalid\n",
    "    }\n",
    "    \n",
    "    return df_clean, log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4869c3fd",
   "metadata": {},
   "source": [
    "## 7. H3 Validation\n",
    "\n",
    "Validate H3 geospatial indexes for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df256689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_h3_indexes(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
    "    \"\"\"Validate H3 geospatial indexes.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 6: H3 VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    print(f\"\\nStarting rows: {initial_rows:,}\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    venue_nulls = df_clean['venue_location_h3_index'].isna().sum()\n",
    "    customer_nulls = df_clean['customer_location_h3_index'].isna().sum()\n",
    "    print(f\"\\nNull values:\")\n",
    "    print(f\"  venue_location_h3_index: {venue_nulls:,}\")\n",
    "    print(f\"  customer_location_h3_index: {customer_nulls:,}\")\n",
    "    \n",
    "    df_clean = df_clean.dropna(subset=['venue_location_h3_index', 'customer_location_h3_index']).copy()\n",
    "    null_removed = venue_nulls + customer_nulls\n",
    "    print(f\"Removed {null_removed:,} rows with null H3 indexes\")\n",
    "    \n",
    "    venue_lengths = df_clean['venue_location_h3_index'].str.len()\n",
    "    customer_lengths = df_clean['customer_location_h3_index'].str.len()\n",
    "    \n",
    "    print(f\"\\nH3 string length distribution:\")\n",
    "    print(f\"  venue_location_h3_index: {venue_lengths.value_counts().to_dict()}\")\n",
    "    print(f\"  customer_location_h3_index: {customer_lengths.value_counts().to_dict()}\")\n",
    "    \n",
    "    expected_length = venue_lengths.mode()[0]\n",
    "    print(f\"\\nExpected H3 length: {expected_length}\")\n",
    "    \n",
    "    before_length_check = len(df_clean)\n",
    "    df_clean = df_clean[\n",
    "        (venue_lengths == expected_length) & \n",
    "        (customer_lengths == expected_length)\n",
    "    ].copy()\n",
    "    length_invalid = before_length_check - len(df_clean)\n",
    "    print(f\"Removed {length_invalid:,} rows with inconsistent H3 lengths\")\n",
    "    \n",
    "    empty_venue = (df_clean['venue_location_h3_index'] == '').sum()\n",
    "    empty_customer = (df_clean['customer_location_h3_index'] == '').sum()\n",
    "    df_clean = df_clean[\n",
    "        (df_clean['venue_location_h3_index'] != '') & \n",
    "        (df_clean['customer_location_h3_index'] != '')\n",
    "    ].copy()\n",
    "    empty_removed = empty_venue + empty_customer\n",
    "    if empty_removed > 0:\n",
    "        print(f\"Removed {empty_removed:,} rows with empty H3 strings\")\n",
    "    \n",
    "    print(f\"\\nUnique locations:\")\n",
    "    print(f\"  Unique venues: {df_clean['venue_location_h3_index'].nunique():,}\")\n",
    "    print(f\"  Unique customers: {df_clean['customer_location_h3_index'].nunique():,}\")\n",
    "    \n",
    "    total_removed = initial_rows - len(df_clean)\n",
    "    print(f\"\\nTotal rows removed: {total_removed:,}\")\n",
    "    print(f\"Remaining rows: {len(df_clean):,}\")\n",
    "    \n",
    "    log = {\n",
    "        'h3_nulls': null_removed,\n",
    "        'h3_length_invalid': length_invalid,\n",
    "        'h3_empty': empty_removed\n",
    "    }\n",
    "    \n",
    "    return df_clean, log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd99ba6e",
   "metadata": {},
   "source": [
    "## 8. Missing Values Handling\n",
    "\n",
    "Handle missing values with explicit strategies for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8b7c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"Handle missing values with explicit strategies.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 7: MISSING VALUES HANDLING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    print(f\"\\nStarting rows: {initial_rows:,}\")\n",
    "    \n",
    "    missing_counts = df.isna().sum()\n",
    "    missing_pct = (missing_counts / len(df) * 100).round(2)\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing Count': missing_counts,\n",
    "        'Missing %': missing_pct\n",
    "    })\n",
    "    missing_summary = missing_summary[missing_summary['Missing Count'] > 0].sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    print(\"\\nMissing value summary:\")\n",
    "    if len(missing_summary) > 0:\n",
    "        print(missing_summary)\n",
    "    else:\n",
    "        print(\"No missing values found!\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    imputation_log = {}\n",
    "    \n",
    "    if 'precipitation' in missing_summary.index:\n",
    "        precip_missing = df_clean['precipitation'].isna().sum()\n",
    "        df_clean['precipitation'] = df_clean['precipitation'].fillna(0)\n",
    "        print(f\"\\nFilled {precip_missing:,} missing precipitation values with 0\")\n",
    "        imputation_log['precipitation_filled'] = precip_missing\n",
    "    \n",
    "    cols_to_drop = []\n",
    "    for col in missing_summary.index:\n",
    "        if col != 'precipitation' and missing_summary.loc[col, 'Missing %'] < 1.0:\n",
    "            if col != 'actual_delivery_time_minutes':\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    if cols_to_drop:\n",
    "        before_drop = len(df_clean)\n",
    "        df_clean = df_clean.dropna(subset=cols_to_drop).copy()\n",
    "        rows_dropped = before_drop - len(df_clean)\n",
    "        print(f\"\\nDropped {rows_dropped:,} rows with missing values in: {cols_to_drop}\")\n",
    "        imputation_log['rows_dropped_missing'] = rows_dropped\n",
    "    \n",
    "    if df_clean['actual_delivery_time_minutes'].isna().sum() > 0:\n",
    "        target_missing = df_clean['actual_delivery_time_minutes'].isna().sum()\n",
    "        df_clean = df_clean.dropna(subset=['actual_delivery_time_minutes']).copy()\n",
    "        print(f\"\\nDropped {target_missing:,} rows with missing target variable\")\n",
    "        imputation_log['target_missing_dropped'] = target_missing\n",
    "    \n",
    "    final_missing = df_clean.isna().sum().sum()\n",
    "    print(f\"\\nFinal missing values: {final_missing}\")\n",
    "    \n",
    "    if final_missing > 0:\n",
    "        print(\"\\nWARNING: Some missing values remain:\")\n",
    "        print(df_clean.isna().sum()[df_clean.isna().sum() > 0])\n",
    "    else:\n",
    "        print(\"No missing values remain\")\n",
    "    \n",
    "    total_removed = initial_rows - len(df_clean)\n",
    "    print(f\"\\nTotal rows removed: {total_removed:,}\")\n",
    "    print(f\"Remaining rows: {len(df_clean):,}\")\n",
    "    \n",
    "    return df_clean, imputation_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e091bc6",
   "metadata": {},
   "source": [
    "## 9. Time-Based Split\n",
    "\n",
    "Create train/validation/test splits based on time ordering to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef63f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_based_split(\n",
    "    df: pd.DataFrame, \n",
    "    train_pct: float = 0.70, \n",
    "    val_pct: float = 0.15,\n",
    "    test_pct: float = 0.15\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Create time-based train/validation/test split to prevent data leakage.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 8: TIME-BASED SPLIT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    assert abs(train_pct + val_pct + test_pct - 1.0) < 1e-6\n",
    "    \n",
    "    df_sorted = df.sort_values('order_placed_at_utc').reset_index(drop=True)\n",
    "    print(f\"\\nSorted {len(df_sorted):,} rows by order_placed_at_utc\")\n",
    "    \n",
    "    n = len(df_sorted)\n",
    "    train_end = int(n * train_pct)\n",
    "    val_end = int(n * (train_pct + val_pct))\n",
    "    \n",
    "    train_df = df_sorted.iloc[:train_end].copy()\n",
    "    val_df = df_sorted.iloc[train_end:val_end].copy()\n",
    "    test_df = df_sorted.iloc[val_end:].copy()\n",
    "    \n",
    "    print(f\"\\nSplit sizes:\")\n",
    "    print(f\"  Train: {len(train_df):,} rows ({len(train_df)/n*100:.1f}%)\")\n",
    "    print(f\"  Val:   {len(val_df):,} rows ({len(val_df)/n*100:.1f}%)\")\n",
    "    print(f\"  Test:  {len(test_df):,} rows ({len(test_df)/n*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nDate ranges:\")\n",
    "    print(f\"  Train: {train_df['order_placed_at_utc'].min()} to {train_df['order_placed_at_utc'].max()}\")\n",
    "    print(f\"  Val:   {val_df['order_placed_at_utc'].min()} to {val_df['order_placed_at_utc'].max()}\")\n",
    "    print(f\"  Test:  {test_df['order_placed_at_utc'].min()} to {test_df['order_placed_at_utc'].max()}\")\n",
    "    \n",
    "    assert train_df['order_placed_at_utc'].max() <= val_df['order_placed_at_utc'].min()\n",
    "    assert val_df['order_placed_at_utc'].max() <= test_df['order_placed_at_utc'].min()\n",
    "    print(\"\\nVerified no temporal overlap between splits\")\n",
    "    \n",
    "    print(\"\\nTarget distribution across splits:\")\n",
    "    print(f\"  Train - Mean: {train_df['actual_delivery_time_minutes'].mean():.2f}, \"\n",
    "          f\"Std: {train_df['actual_delivery_time_minutes'].std():.2f}\")\n",
    "    print(f\"  Val   - Mean: {val_df['actual_delivery_time_minutes'].mean():.2f}, \"\n",
    "          f\"Std: {val_df['actual_delivery_time_minutes'].std():.2f}\")\n",
    "    print(f\"  Test  - Mean: {test_df['actual_delivery_time_minutes'].mean():.2f}, \"\n",
    "          f\"Std: {test_df['actual_delivery_time_minutes'].std():.2f}\")\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecec9df",
   "metadata": {},
   "source": [
    "## 10. Main Pipeline Function\n",
    "\n",
    "Orchestrate the entire cleaning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cleaning_pipeline(\n",
    "    filepath: str,\n",
    "    output_dir: str = '../outputs/'\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Execute the complete data cleaning pipeline.\"\"\"\n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(\"#\" + \" \"*78 + \"#\")\n",
    "    print(\"#\" + \" \"*20 + \"DATA CLEANING PIPELINE\" + \" \"*36 + \"#\")\n",
    "    print(\"#\" + \" \"*78 + \"#\")\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    pipeline_log = {\n",
    "        'input_file': filepath,\n",
    "        'pipeline_start': datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "    \n",
    "    df = load_and_enforce_types(filepath)\n",
    "    pipeline_log['initial_rows'] = len(df)\n",
    "    pipeline_log['date_range_start'] = df['order_placed_at_utc'].min().isoformat()\n",
    "    pipeline_log['date_range_end'] = df['order_placed_at_utc'].max().isoformat()\n",
    "    \n",
    "    df, dup_log = remove_duplicates(df)\n",
    "    pipeline_log.update(dup_log)\n",
    "    \n",
    "    df, target_log = clean_target(df)\n",
    "    pipeline_log.update(target_log)\n",
    "    \n",
    "    df, eta_log = validate_and_engineer_eta(df)\n",
    "    pipeline_log.update(eta_log)\n",
    "    \n",
    "    df, feature_log = validate_features(df)\n",
    "    pipeline_log.update(feature_log)\n",
    "    \n",
    "    df, h3_log = validate_h3_indexes(df)\n",
    "    pipeline_log.update(h3_log)\n",
    "    \n",
    "    df, missing_log = handle_missing_values(df)\n",
    "    pipeline_log.update(missing_log)\n",
    "    \n",
    "    pipeline_log['final_rows'] = len(df)\n",
    "    pipeline_log['total_rows_removed'] = pipeline_log['initial_rows'] - pipeline_log['final_rows']\n",
    "    pipeline_log['retention_rate'] = f\"{len(df) / pipeline_log['initial_rows'] * 100:.2f}%\"\n",
    "    \n",
    "    train_df, val_df, test_df = create_time_based_split(df)\n",
    "    pipeline_log['train_rows'] = len(train_df)\n",
    "    pipeline_log['val_rows'] = len(val_df)\n",
    "    pipeline_log['test_rows'] = len(test_df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAVING CLEANED DATASETS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    full_path = os.path.join(output_dir, 'cleaned_full.csv')\n",
    "    train_path = os.path.join(output_dir, 'train.csv')\n",
    "    val_path = os.path.join(output_dir, 'val.csv')\n",
    "    test_path = os.path.join(output_dir, 'test.csv')\n",
    "    \n",
    "    df.to_csv(full_path, index=False)\n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    val_df.to_csv(val_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "    \n",
    "    print(f\"\\nSaved cleaned_full.csv ({len(df):,} rows)\")\n",
    "    print(f\"Saved train.csv ({len(train_df):,} rows)\")\n",
    "    print(f\"Saved val.csv ({len(val_df):,} rows)\")\n",
    "    print(f\"Saved test.csv ({len(test_df):,} rows)\")\n",
    "    print(f\"\\nAll files saved to: {output_dir}\")\n",
    "    \n",
    "    pipeline_log['pipeline_end'] = datetime.now(timezone.utc).isoformat()\n",
    "    \n",
    "    return {\n",
    "        'cleaned_df': df,\n",
    "        'train_df': train_df,\n",
    "        'val_df': val_df,\n",
    "        'test_df': test_df,\n",
    "        'log': pipeline_log\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38da2eb",
   "metadata": {},
   "source": [
    "## 11. Execute Pipeline\n",
    "\n",
    "Run the complete data cleaning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df60f4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "#                                                                              #\n",
      "#                    DATA CLEANING PIPELINE                                    #\n",
      "#                                                                              #\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "STEP 1: DATA LOADING AND TYPE ENFORCEMENT\n",
      "================================================================================\n",
      "\n",
      "Loaded 24,942 rows from ../data/orders_spring_2022.csv\n",
      "✓ Localized timestamps to UTC\n",
      "\n",
      "✓ Type enforcement complete\n",
      "\n",
      "Data types:\n",
      "order_placed_at_utc                      datetime64[ns, UTC]\n",
      "order_category                                      category\n",
      "item_count                                             Int64\n",
      "actual_delivery_time_minutes                         float64\n",
      "estimated_delivery_time_lower_minutes                float64\n",
      "estimated_delivery_time_upper_minutes                float64\n",
      "venue_location_h3_index                               object\n",
      "customer_location_h3_index                            object\n",
      "courier_supply_index                                 float64\n",
      "precipitation                                        float64\n",
      "dtype: object\n",
      "\n",
      "Date range: 2022-02-01 06:01:00+00:00 to 2022-04-29 23:50:00+00:00\n",
      "Number of days: 87\n",
      "\n",
      "================================================================================\n",
      "STEP 2: STRUCTURAL VALIDATION (DUPLICATES)\n",
      "================================================================================\n",
      "\n",
      "Starting rows: 24,942\n",
      "\n",
      "✓ Removed 0 exact duplicate rows\n",
      "✓ Removed 0 potential duplicate orders\n",
      "\n",
      "Total rows removed: 0\n",
      "Remaining rows: 24,942\n",
      "\n",
      "================================================================================\n",
      "STEP 3: TARGET CLEANING\n",
      "================================================================================\n",
      "\n",
      "Starting rows: 24,942\n",
      "\n",
      "Target statistics BEFORE cleaning:\n",
      "count    24942.00\n",
      "mean        34.66\n",
      "std        120.37\n",
      "min          0.02\n",
      "25%         22.09\n",
      "50%         29.73\n",
      "75%         40.19\n",
      "max      15631.06\n",
      "Name: actual_delivery_time_minutes, dtype: float64\n",
      "Missing values: 0\n",
      "\n",
      "✓ Removed 0 rows with actual_delivery_time_minutes <= 0\n",
      "✓ Removed 26 rows with actual_delivery_time_minutes > 180\n",
      "\n",
      "Target statistics AFTER cleaning:\n",
      "count    24916.00\n",
      "mean        32.88\n",
      "std         15.68\n",
      "min          0.02\n",
      "25%         22.08\n",
      "50%         29.71\n",
      "75%         40.16\n",
      "max        178.37\n",
      "Name: actual_delivery_time_minutes, dtype: float64\n",
      "\n",
      "Total rows removed: 26\n",
      "Remaining rows: 24,916\n",
      "\n",
      "================================================================================\n",
      "STEP 4: ETA CONSISTENCY CHECKS\n",
      "================================================================================\n",
      "\n",
      "Starting rows: 24,916\n",
      "\n",
      "✓ Removed 36 rows with estimated_delivery_time_lower_minutes <= 0\n",
      "✓ Removed 0 rows with estimated_delivery_time_upper_minutes <= 0\n",
      "✓ Removed 0 rows where lower > upper\n",
      "\n",
      "✓ Created derived features: estimate_width, estimate_midpoint\n",
      "\n",
      "Estimate width statistics:\n",
      "count    24824.00\n",
      "mean        10.14\n",
      "std          1.59\n",
      "min          5.00\n",
      "25%         10.00\n",
      "50%         10.00\n",
      "75%         10.00\n",
      "max         40.00\n",
      "Name: estimate_width, dtype: float64\n",
      "\n",
      "Rows with zero estimate width: 0 (0.00%)\n",
      "Rows with estimate width > 180 minutes: 0 (0.00%)\n",
      "(These rows are flagged but not removed unless clearly invalid)\n",
      "\n",
      "Total rows removed: 92\n",
      "Remaining rows: 24,824\n",
      "\n",
      "================================================================================\n",
      "STEP 5: FEATURE SANITY CHECKS\n",
      "================================================================================\n",
      "\n",
      "Starting rows: 24,824\n",
      "\n",
      "--- courier_supply_index ---\n",
      "Range: [1.00, 2.64]\n",
      "✓ Removed 0 rows with courier_supply_index < 0\n",
      "\n",
      "--- precipitation ---\n",
      "Range: [0.00, 3.40]\n",
      "✓ Removed 0 rows with precipitation < 0\n",
      "✓ Removed 0 rows with precipitation > 100\n",
      "\n",
      "--- item_count ---\n",
      "Range: [1, 100]\n",
      "✓ Removed 0 rows with item_count <= 0\n",
      "\n",
      "Numeric feature summary:\n",
      "       courier_supply_index  precipitation  item_count  \\\n",
      "count              23865.00       23865.00     23865.0   \n",
      "mean                   1.74           0.10        3.53   \n",
      "std                    0.24           0.36        4.89   \n",
      "min                    1.00           0.00         1.0   \n",
      "25%                    1.57           0.00         1.0   \n",
      "50%                    1.74           0.00         2.0   \n",
      "75%                    1.90           0.00         4.0   \n",
      "max                    2.64           3.40       100.0   \n",
      "\n",
      "       actual_delivery_time_minutes  \n",
      "count                      23865.00  \n",
      "mean                          32.90  \n",
      "std                           15.70  \n",
      "min                            0.02  \n",
      "25%                           22.10  \n",
      "50%                           29.70  \n",
      "75%                           40.11  \n",
      "max                          178.37  \n",
      "\n",
      "Total rows removed: 959\n",
      "Remaining rows: 23,865\n",
      "\n",
      "================================================================================\n",
      "STEP 6: H3 VALIDATION\n",
      "================================================================================\n",
      "\n",
      "Starting rows: 23,865\n",
      "\n",
      "Null values:\n",
      "  venue_location_h3_index: 0\n",
      "  customer_location_h3_index: 0\n",
      "✓ Removed 0 rows with null H3 indexes\n",
      "\n",
      "H3 string length distribution:\n",
      "  venue_location_h3_index: {15: 23865}\n",
      "  customer_location_h3_index: {15: 23865}\n",
      "\n",
      "Expected H3 length: 15\n",
      "✓ Removed 0 rows with inconsistent H3 lengths\n",
      "\n",
      "Unique locations:\n",
      "  Unique venues: 143\n",
      "  Unique customers: 378\n",
      "\n",
      "Total rows removed: 0\n",
      "Remaining rows: 23,865\n",
      "\n",
      "================================================================================\n",
      "STEP 7: MISSING VALUES HANDLING\n",
      "================================================================================\n",
      "\n",
      "Starting rows: 23,865\n",
      "\n",
      "Missing value summary:\n",
      "No missing values found!\n",
      "\n",
      "Final missing values: 0\n",
      "✓ No missing values remain\n",
      "\n",
      "Total rows removed: 0\n",
      "Remaining rows: 23,865\n",
      "\n",
      "================================================================================\n",
      "STEP 8: TIME-BASED SPLIT\n",
      "================================================================================\n",
      "\n",
      "✓ Sorted 23,865 rows by order_placed_at_utc\n",
      "\n",
      "Split sizes:\n",
      "  Train: 16,705 rows (70.0%)\n",
      "  Val:   3,580 rows (15.0%)\n",
      "  Test:  3,580 rows (15.0%)\n",
      "\n",
      "Date ranges:\n",
      "  Train: 2022-02-01 06:01:00+00:00 to 2022-04-03 14:30:00+00:00\n",
      "  Val:   2022-04-03 14:34:00+00:00 to 2022-04-16 17:29:00+00:00\n",
      "  Test:  2022-04-16 17:32:00+00:00 to 2022-04-29 23:50:00+00:00\n",
      "\n",
      "✓ Verified no temporal overlap between splits\n",
      "\n",
      "Target distribution across splits:\n",
      "  Train - Mean: 34.21, Std: 16.24\n",
      "  Val   - Mean: 30.09, Std: 13.63\n",
      "  Test  - Mean: 29.59, Std: 14.19\n",
      "\n",
      "================================================================================\n",
      "SAVING CLEANED DATASETS\n",
      "================================================================================\n",
      "\n",
      "✓ Saved cleaned_full.csv (23,865 rows)\n",
      "✓ Saved train.csv (16,705 rows)\n",
      "✓ Saved val.csv (3,580 rows)\n",
      "✓ Saved test.csv (3,580 rows)\n",
      "\n",
      "All files saved to: ../outputs/\n"
     ]
    }
   ],
   "source": [
    "# Execute the pipeline\n",
    "results = run_cleaning_pipeline(\n",
    "    filepath='../data/orders_spring_2022.csv',\n",
    "    output_dir='../outputs/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27773d9b",
   "metadata": {},
   "source": [
    "## 12. Pipeline Summary Report\n",
    "\n",
    "Display comprehensive summary of the cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "#                                                                              #\n",
      "#                         PIPELINE SUMMARY                                     #\n",
      "#                                                                              #\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "DATASET INFORMATION\n",
      "================================================================================\n",
      "Input file: ../data/orders_spring_2022.csv\n",
      "Date range: 2022-02-01 to 2022-04-29\n",
      "Initial rows: 24,942\n",
      "Final rows: 23,865\n",
      "Rows removed: 1,077\n",
      "Retention rate: 95.68%\n",
      "\n",
      "================================================================================\n",
      "ROWS REMOVED PER RULE\n",
      "================================================================================\n",
      "  Target unrealistic (>180 min)...............................       26\n",
      "  ETA lower invalid...........................................       36\n",
      "\n",
      "  TOTAL.......................................................       62\n",
      "\n",
      "================================================================================\n",
      "TRAIN/VAL/TEST SPLIT\n",
      "================================================================================\n",
      "  Training set:     16,705 rows ( 70.0%)\n",
      "  Validation set:    3,580 rows ( 15.0%)\n",
      "  Test set:          3,580 rows ( 15.0%)\n",
      "\n",
      "================================================================================\n",
      "KEY FEATURES ENGINEERED\n",
      "================================================================================\n",
      "  • estimate_width: Upper bound - Lower bound\n",
      "  • estimate_midpoint: (Upper bound + Lower bound) / 2\n",
      "\n",
      "  Rows with zero estimate width: 0\n",
      "  Rows with large estimate width (>180 min): 0\n",
      "\n",
      "================================================================================\n",
      "PIPELINE STATUS\n",
      "================================================================================\n",
      "✓ Data loading and type enforcement\n",
      "✓ Structural validation (duplicates)\n",
      "✓ Target variable cleaning\n",
      "✓ ETA consistency validation\n",
      "✓ Feature sanity checks\n",
      "✓ H3 geospatial index validation\n",
      "✓ Missing value handling\n",
      "✓ Time-based train/val/test split\n",
      "\n",
      "✓✓✓ PIPELINE COMPLETED SUCCESSFULLY ✓✓✓\n",
      "\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_pipeline_summary(log: Dict[str, Any]):\n",
    "    \"\"\"Print a comprehensive summary of the cleaning pipeline.\"\"\"\n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(\"#\" + \" \"*78 + \"#\")\n",
    "    print(\"#\" + \" \"*25 + \"PIPELINE SUMMARY\" + \" \"*37 + \"#\")\n",
    "    print(\"#\" + \" \"*78 + \"#\")\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATASET INFORMATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Input file: {log['input_file']}\")\n",
    "    print(f\"Date range: {log['date_range_start'][:10]} to {log['date_range_end'][:10]}\")\n",
    "    print(f\"Initial rows: {log['initial_rows']:,}\")\n",
    "    print(f\"Final rows: {log['final_rows']:,}\")\n",
    "    print(f\"Rows removed: {log['total_rows_removed']:,}\")\n",
    "    print(f\"Retention rate: {log['retention_rate']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ROWS REMOVED PER RULE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    removal_reasons = [\n",
    "        ('Exact duplicates', 'exact_duplicates'),\n",
    "        ('Potential duplicate orders', 'potential_duplicates'),\n",
    "        ('Target non-positive', 'target_non_positive'),\n",
    "        ('Target unrealistic (>180 min)', 'target_unrealistic'),\n",
    "        ('ETA lower invalid', 'eta_lower_invalid'),\n",
    "        ('ETA upper invalid', 'eta_upper_invalid'),\n",
    "        ('ETA inconsistent (lower>upper)', 'eta_inconsistent'),\n",
    "        ('Courier supply invalid', 'courier_supply_invalid'),\n",
    "        ('Precipitation negative', 'precipitation_negative'),\n",
    "        ('Precipitation excessive', 'precipitation_excessive'),\n",
    "        ('Item count invalid', 'item_count_invalid'),\n",
    "        ('H3 nulls', 'h3_nulls'),\n",
    "        ('H3 length invalid', 'h3_length_invalid'),\n",
    "        ('H3 empty strings', 'h3_empty'),\n",
    "    ]\n",
    "    \n",
    "    total_logged = 0\n",
    "    for label, key in removal_reasons:\n",
    "        if key in log:\n",
    "            count = log[key]\n",
    "            if count > 0:\n",
    "                print(f\"  {label:.<60} {count:>8,}\")\n",
    "                total_logged += count\n",
    "    \n",
    "    if 'precipitation_filled' in log:\n",
    "        print(f\"\\n  {'Precipitation values filled with 0':.<60} {log['precipitation_filled']:>8,}\")\n",
    "    if 'rows_dropped_missing' in log:\n",
    "        print(f\"  {'Rows dropped for other missing values':.<60} {log['rows_dropped_missing']:>8,}\")\n",
    "        total_logged += log['rows_dropped_missing']\n",
    "    if 'target_missing_dropped' in log:\n",
    "        print(f\"  {'Rows dropped for missing target':.<60} {log['target_missing_dropped']:>8,}\")\n",
    "        total_logged += log['target_missing_dropped']\n",
    "    \n",
    "    print(f\"\\n  {'TOTAL':.<60} {total_logged:>8,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAIN/VAL/TEST SPLIT\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  Training set:   {log['train_rows']:>8,} rows ({log['train_rows']/log['final_rows']*100:>5.1f}%)\")\n",
    "    print(f\"  Validation set: {log['val_rows']:>8,} rows ({log['val_rows']/log['final_rows']*100:>5.1f}%)\")\n",
    "    print(f\"  Test set:       {log['test_rows']:>8,} rows ({log['test_rows']/log['final_rows']*100:>5.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"KEY FEATURES ENGINEERED\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"  - estimate_width: Upper bound - Lower bound\")\n",
    "    print(\"  - estimate_midpoint: (Upper bound + Lower bound) / 2\")\n",
    "    \n",
    "    if 'zero_width_count' in log:\n",
    "        print(f\"\\n  Rows with zero estimate width: {log['zero_width_count']:,}\")\n",
    "    if 'large_width_count' in log:\n",
    "        print(f\"  Rows with large estimate width (>180 min): {log['large_width_count']:,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PIPELINE STATUS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"- Data loading and type enforcement\")\n",
    "    print(\"- Structural validation (duplicates)\")\n",
    "    print(\"- Target variable cleaning\")\n",
    "    print(\"- ETA consistency validation\")\n",
    "    print(\"- Feature sanity checks\")\n",
    "    print(\"- H3 geospatial index validation\")\n",
    "    print(\"- Missing value handling\")\n",
    "    print(\"- Time-based train/val/test split\")\n",
    "    print(\"\\nPIPELINE COMPLETED SUCCESSFULLY\")\n",
    "    \n",
    "    print(\"\\n\" + \"#\"*80 + \"\\n\")\n",
    "\n",
    "print_pipeline_summary(results['log'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109cea5b",
   "metadata": {},
   "source": [
    "## 13. Final Data Quality Checks\n",
    "\n",
    "Verify the cleaned dataset meets all quality criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65073a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Quality Checks\n",
      "================================================================================\n",
      "\n",
      "1. Missing values: 0 ✓\n",
      "2. Target in valid range (0, 180]: ✓\n",
      "3. ETA estimates valid and consistent: ✓\n",
      "4. All features in valid ranges: ✓\n",
      "5. H3 indexes valid and consistent: ✓\n",
      "6. No duplicate rows: ✓\n",
      "7. Splits properly time-ordered: ✓\n",
      "8. Split sizes sum to total: ✓\n",
      "\n",
      "================================================================================\n",
      "\n",
      "✓✓✓ ALL QUALITY CHECKS PASSED ✓✓✓\n",
      "\n",
      "Dataset is ready for model training!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = results['cleaned_df']\n",
    "train_df = results['train_df']\n",
    "val_df = results['val_df']\n",
    "test_df = results['test_df']\n",
    "\n",
    "print(\"Final Quality Checks\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "missing_count = cleaned_df.isna().sum().sum()\n",
    "print(f\"\\n1. Missing values: {missing_count} {'PASS' if missing_count == 0 else 'FAIL'}\")\n",
    "\n",
    "target_valid = (\n",
    "    (cleaned_df['actual_delivery_time_minutes'] > 0).all() and\n",
    "    (cleaned_df['actual_delivery_time_minutes'] <= 180).all()\n",
    ")\n",
    "print(f\"2. Target in valid range (0, 180]: {'PASS' if target_valid else 'FAIL'}\")\n",
    "\n",
    "eta_valid = (\n",
    "    (cleaned_df['estimated_delivery_time_lower_minutes'] > 0).all() and\n",
    "    (cleaned_df['estimated_delivery_time_upper_minutes'] > 0).all() and\n",
    "    (cleaned_df['estimated_delivery_time_lower_minutes'] <= \n",
    "     cleaned_df['estimated_delivery_time_upper_minutes']).all()\n",
    ")\n",
    "print(f\"3. ETA estimates valid and consistent: {'PASS' if eta_valid else 'FAIL'}\")\n",
    "\n",
    "features_valid = (\n",
    "    (cleaned_df['courier_supply_index'] >= 0).all() and\n",
    "    (cleaned_df['precipitation'] >= 0).all() and\n",
    "    (cleaned_df['precipitation'] <= 100).all() and\n",
    "    (cleaned_df['item_count'] > 0).all()\n",
    ")\n",
    "print(f\"4. All features in valid ranges: {'PASS' if features_valid else 'FAIL'}\")\n",
    "\n",
    "h3_valid = (\n",
    "    cleaned_df['venue_location_h3_index'].notna().all() and\n",
    "    cleaned_df['customer_location_h3_index'].notna().all() and\n",
    "    (cleaned_df['venue_location_h3_index'].str.len().nunique() == 1) and\n",
    "    (cleaned_df['customer_location_h3_index'].str.len().nunique() == 1)\n",
    ")\n",
    "print(f\"5. H3 indexes valid and consistent: {'PASS' if h3_valid else 'FAIL'}\")\n",
    "\n",
    "no_duplicates = not cleaned_df.duplicated().any()\n",
    "print(f\"6. No duplicate rows: {'PASS' if no_duplicates else 'FAIL'}\")\n",
    "\n",
    "time_ordered = (\n",
    "    train_df['order_placed_at_utc'].max() <= val_df['order_placed_at_utc'].min() and\n",
    "    val_df['order_placed_at_utc'].max() <= test_df['order_placed_at_utc'].min()\n",
    ")\n",
    "print(f\"7. Splits properly time-ordered: {'PASS' if time_ordered else 'FAIL'}\")\n",
    "\n",
    "total_split = len(train_df) + len(val_df) + len(test_df)\n",
    "splits_match = total_split == len(cleaned_df)\n",
    "print(f\"8. Split sizes sum to total: {'PASS' if splits_match else 'FAIL'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if all([missing_count == 0, target_valid, eta_valid, features_valid, \n",
    "        h3_valid, no_duplicates, time_ordered, splits_match]):\n",
    "    print(\"\\nALL QUALITY CHECKS PASSED\")\n",
    "    print(\"\\nDataset is ready for model training!\")\n",
    "else:\n",
    "    print(\"\\nWARNING: SOME QUALITY CHECKS FAILED - REVIEW REQUIRED\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3a59cf",
   "metadata": {},
   "source": [
    "## 14. Data Overview and Statistics\n",
    "\n",
    "Display final dataset characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5883696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Dataset Overview\n",
      "================================================================================\n",
      "\n",
      "Dataset Shape:\n",
      "  Rows: 23,865\n",
      "  Columns: 12\n",
      "\n",
      "Column Names:\n",
      "  order_placed_at_utc, order_category, item_count, actual_delivery_time_minutes, estimated_delivery_time_lower_minutes, estimated_delivery_time_upper_minutes, venue_location_h3_index, customer_location_h3_index, courier_supply_index, precipitation, estimate_width, estimate_midpoint\n",
      "\n",
      "Data Types:\n",
      "order_placed_at_utc                      datetime64[ns, UTC]\n",
      "order_category                                      category\n",
      "item_count                                             Int64\n",
      "actual_delivery_time_minutes                         float64\n",
      "estimated_delivery_time_lower_minutes                float64\n",
      "estimated_delivery_time_upper_minutes                float64\n",
      "venue_location_h3_index                               object\n",
      "customer_location_h3_index                            object\n",
      "courier_supply_index                                 float64\n",
      "precipitation                                        float64\n",
      "estimate_width                                       float64\n",
      "estimate_midpoint                                    float64\n",
      "dtype: object\n",
      "\n",
      "Numerical Features Summary:\n",
      "       item_count  actual_delivery_time_minutes  \\\n",
      "count     23865.0                      23865.00   \n",
      "mean         3.53                         32.90   \n",
      "std          4.89                         15.70   \n",
      "min           1.0                          0.02   \n",
      "25%           1.0                         22.10   \n",
      "50%           2.0                         29.70   \n",
      "75%           4.0                         40.11   \n",
      "max         100.0                        178.37   \n",
      "\n",
      "       estimated_delivery_time_lower_minutes  \\\n",
      "count                               23865.00   \n",
      "mean                                   25.42   \n",
      "std                                    10.94   \n",
      "min                                     5.00   \n",
      "25%                                    20.00   \n",
      "50%                                    25.00   \n",
      "75%                                    30.00   \n",
      "max                                    80.00   \n",
      "\n",
      "       estimated_delivery_time_upper_minutes  courier_supply_index  \\\n",
      "count                               23865.00              23865.00   \n",
      "mean                                   35.56                  1.74   \n",
      "std                                    10.94                  0.24   \n",
      "min                                    10.00                  1.00   \n",
      "25%                                    30.00                  1.57   \n",
      "50%                                    35.00                  1.74   \n",
      "75%                                    40.00                  1.90   \n",
      "max                                    90.00                  2.64   \n",
      "\n",
      "       precipitation  estimate_width  estimate_midpoint  \n",
      "count       23865.00        23865.00           23865.00  \n",
      "mean            0.10           10.14              30.49  \n",
      "std             0.36            1.60              10.91  \n",
      "min             0.00            5.00               7.50  \n",
      "25%             0.00           10.00              25.00  \n",
      "50%             0.00           10.00              30.00  \n",
      "75%             0.00           10.00              35.00  \n",
      "max             3.40           40.00              85.00  \n",
      "\n",
      "Categorical Feature:\n",
      "  order_category value counts:\n",
      "order_category\n",
      "Food delivery    20906\n",
      "Retail            2959\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Memory Usage:\n",
      "  4.78 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCleaned Dataset Overview\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nDataset Shape:\")\n",
    "print(f\"  Rows: {len(cleaned_df):,}\")\n",
    "print(f\"  Columns: {len(cleaned_df.columns)}\")\n",
    "\n",
    "print(\"\\nColumn Names:\")\n",
    "print(f\"  {', '.join(cleaned_df.columns)}\")\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(cleaned_df.dtypes)\n",
    "\n",
    "print(\"\\nNumerical Features Summary:\")\n",
    "print(cleaned_df.describe())\n",
    "\n",
    "print(\"\\nCategorical Feature:\")\n",
    "print(f\"  order_category value counts:\")\n",
    "print(cleaned_df['order_category'].value_counts())\n",
    "\n",
    "print(\"\\nMemory Usage:\")\n",
    "memory_mb = cleaned_df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "print(f\"  {memory_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4786d",
   "metadata": {},
   "source": [
    "## 14.5. Feature Engineering\n",
    "\n",
    "Create additional predictive features to improve model performance:\n",
    "1. **H3 Distance**: Calculate geographic distance between venue and customer locations\n",
    "2. **Venue Friction Score**: Identify venues with historically longer delivery times using target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c22731",
   "metadata": {},
   "source": [
    "### 14.5.1 H3 Distance Feature\n",
    "\n",
    "Calculate geographic distance between venue and customer using H3 hexagonal indexes.\n",
    "Distance is converted from H3 grid distance to approximate kilometers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e08d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING: H3 DISTANCE\n",
      "================================================================================\n",
      "Calculating H3 distance features...\n",
      "✓ H3 distance features created\n",
      "  - h3_grid_distance (hexagon units)\n",
      "  - h3_distance_km (approximate delivery distance)\n",
      "\n",
      "Distance Statistics:\n",
      "  Mean: 0.91 km\n",
      "  Median: 0.50 km\n",
      "  Max: 7.00 km\n",
      "  Same location (0 km): 2,905 orders\n",
      "\n",
      "✓ Applied to training set (16,705 rows)\n",
      "Calculating H3 distance features...\n",
      "✓ H3 distance features created\n",
      "  - h3_grid_distance (hexagon units)\n",
      "  - h3_distance_km (approximate delivery distance)\n",
      "\n",
      "Distance Statistics:\n",
      "  Mean: 0.94 km\n",
      "  Median: 0.50 km\n",
      "  Max: 6.00 km\n",
      "  Same location (0 km): 625 orders\n",
      "✓ Applied to validation set (3,580 rows)\n",
      "Calculating H3 distance features...\n",
      "✓ H3 distance features created\n",
      "  - h3_grid_distance (hexagon units)\n",
      "  - h3_distance_km (approximate delivery distance)\n",
      "\n",
      "Distance Statistics:\n",
      "  Mean: 0.94 km\n",
      "  Median: 0.50 km\n",
      "  Max: 5.50 km\n",
      "  Same location (0 km): 612 orders\n",
      "✓ Applied to test set (3,580 rows)\n",
      "Calculating H3 distance features...\n",
      "✓ H3 distance features created\n",
      "  - h3_grid_distance (hexagon units)\n",
      "  - h3_distance_km (approximate delivery distance)\n",
      "\n",
      "Distance Statistics:\n",
      "  Mean: 0.92 km\n",
      "  Median: 0.50 km\n",
      "  Max: 7.00 km\n",
      "  Same location (0 km): 4,142 orders\n",
      "✓ Applied to full cleaned dataset (23,865 rows)\n"
     ]
    }
   ],
   "source": [
    "def calculate_h3_distance_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate distance features from H3 geospatial indexes.\"\"\"\n",
    "    try:\n",
    "        import h3\n",
    "        \n",
    "        print(\"Calculating H3 distance features...\")\n",
    "        \n",
    "        df['h3_grid_distance'] = df.apply(\n",
    "            lambda row: h3.grid_distance(\n",
    "                row['venue_location_h3_index'],\n",
    "                row['customer_location_h3_index']\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        df['h3_distance_km'] = df['h3_grid_distance'] * 0.5\n",
    "        \n",
    "        print(f\"H3 distance features created\")\n",
    "        print(f\"  - h3_grid_distance (hexagon units)\")\n",
    "        print(f\"  - h3_distance_km (approximate delivery distance)\")\n",
    "        print(f\"\\nDistance Statistics:\")\n",
    "        print(f\"  Mean: {df['h3_distance_km'].mean():.2f} km\")\n",
    "        print(f\"  Median: {df['h3_distance_km'].median():.2f} km\")\n",
    "        print(f\"  Max: {df['h3_distance_km'].max():.2f} km\")\n",
    "        print(f\"  Same location (0 km): {(df['h3_distance_km'] == 0).sum():,} orders\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"WARNING: h3 library not available. Install with: pip install h3\")\n",
    "        print(\"Skipping H3 distance features\")\n",
    "        df['h3_grid_distance'] = 0\n",
    "        df['h3_distance_km'] = 0.0\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Error calculating H3 distances: {e}\")\n",
    "        df['h3_grid_distance'] = 0\n",
    "        df['h3_distance_km'] = 0.0\n",
    "        return df\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING: H3 DISTANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_df = calculate_h3_distance_features(train_df)\n",
    "print(f\"\\nApplied to training set ({len(train_df):,} rows)\")\n",
    "\n",
    "val_df = calculate_h3_distance_features(val_df)\n",
    "print(f\"Applied to validation set ({len(val_df):,} rows)\")\n",
    "\n",
    "test_df = calculate_h3_distance_features(test_df)\n",
    "print(f\"Applied to test set ({len(test_df):,} rows)\")\n",
    "\n",
    "cleaned_df = calculate_h3_distance_features(cleaned_df)\n",
    "print(f\"Applied to full cleaned dataset ({len(cleaned_df):,} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c20fd8",
   "metadata": {},
   "source": [
    "### 14.5.2 Venue Friction Score (Target Encoding)\n",
    "\n",
    "Calculate venue-specific delay patterns using historical data to capture systematic delays from certain venues.\n",
    "\n",
    "**Methodology:**\n",
    "1. Calculate error = actual_delivery_time - estimated_delivery_time_upper\n",
    "2. Identify delayed orders (error > 0)\n",
    "3. For each venue, calculate:\n",
    "   - **Mean Extra Time**: Average delay in minutes when delayed\n",
    "   - **Delay Rate**: Percentage of orders that were delayed\n",
    "4. **Friction Score** = Delay Rate × Mean Extra Time\n",
    "\n",
    "This captures venues that are consistently problematic (high delay rate AND high delay magnitude).\n",
    "\n",
    "**Important:** Use training data only to avoid data leakage, then apply to val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8dff48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING: VENUE FRICTION SCORE\n",
      "================================================================================\n",
      "Calculating venue friction scores from training data...\n",
      "✓ Calculated friction scores for 96 venues\n",
      "  (Venues with at least 10 orders)\n",
      "\n",
      "Friction Score Statistics:\n",
      "  Mean: 4.189\n",
      "  Median: 3.791\n",
      "  75th percentile: 4.799\n",
      "  90th percentile: 6.776\n",
      "  95th percentile: 8.060\n",
      "  Max: 9.490\n",
      "\n",
      "Top 10 Venues with Highest Friction Scores:\n",
      "                         delay_rate_pct  mean_delay_when_late  friction_score  order_count\n",
      "venue_location_h3_index                                                                   \n",
      "881126d221fffff                    51.9                 18.28            9.49          131\n",
      "881126d229fffff                    38.7                 23.80            9.21           31\n",
      "881126d0cdfffff                    38.9                 22.49            8.75           18\n",
      "881126d18bfffff                    40.0                 20.96            8.38           70\n",
      "881126d233fffff                    35.7                 22.73            8.12           14\n",
      "881126d00dfffff                    54.5                 14.74            8.04           44\n",
      "881126d2e5fffff                    57.6                 13.22            7.61           33\n",
      "881126d0cbfffff                    44.2                 16.90            7.48           52\n",
      "881126d2a5fffff                    30.0                 23.51            7.05           10\n",
      "881126d063fffff                    52.6                 13.19            6.94           38\n",
      "\n",
      "Applying venue friction scores...\n",
      "✓ Training set: 16,705 rows\n",
      "  - Known venues: 16,544\n",
      "  - Unknown venues (score=0): 161\n",
      "✓ Validation set: 3,580 rows\n",
      "  - Known venues: 3,527\n",
      "  - Unknown venues (score=0): 53\n",
      "✓ Test set: 3,580 rows\n",
      "  - Known venues: 3,525\n",
      "  - Unknown venues (score=0): 55\n",
      "✓ Full cleaned dataset: 23,865 rows\n",
      "\n",
      "✓ Venue friction score feature engineering complete!\n"
     ]
    }
   ],
   "source": [
    "def calculate_venue_friction_score(train_df: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"Calculate venue friction scores from training data using target encoding.\"\"\"\n",
    "    print(\"Calculating venue friction scores from training data...\")\n",
    "    \n",
    "    train_df['delivery_error'] = (\n",
    "        train_df['actual_delivery_time_minutes'] - \n",
    "        train_df['estimated_delivery_time_upper_minutes']\n",
    "    )\n",
    "    \n",
    "    train_df['is_delayed'] = (train_df['delivery_error'] > 0).astype(int)\n",
    "    \n",
    "    venue_stats = train_df.groupby('venue_location_h3_index').agg({\n",
    "        'delivery_error': ['mean', 'count'],\n",
    "        'is_delayed': 'mean'\n",
    "    })\n",
    "    \n",
    "    venue_stats.columns = ['mean_extra_time', 'order_count', 'delay_rate']\n",
    "    \n",
    "    min_orders = 10\n",
    "    venue_stats = venue_stats[venue_stats['order_count'] >= min_orders]\n",
    "    \n",
    "    delayed_only = train_df[train_df['is_delayed'] == 1].groupby('venue_location_h3_index')['delivery_error'].mean()\n",
    "    venue_stats['mean_delay_when_late'] = delayed_only\n",
    "    venue_stats['mean_delay_when_late'] = venue_stats['mean_delay_when_late'].fillna(0)\n",
    "    \n",
    "    venue_stats['friction_score'] = (\n",
    "        venue_stats['delay_rate'] * venue_stats['mean_delay_when_late']\n",
    "    )\n",
    "    \n",
    "    venue_stats['overall_mean_error'] = venue_stats['mean_extra_time']\n",
    "    \n",
    "    print(f\"Calculated friction scores for {len(venue_stats):,} venues\")\n",
    "    print(f\"  (Venues with at least {min_orders} orders)\")\n",
    "    \n",
    "    print(f\"\\nFriction Score Statistics:\")\n",
    "    print(f\"  Mean: {venue_stats['friction_score'].mean():.3f}\")\n",
    "    print(f\"  Median: {venue_stats['friction_score'].median():.3f}\")\n",
    "    print(f\"  75th percentile: {venue_stats['friction_score'].quantile(0.75):.3f}\")\n",
    "    print(f\"  90th percentile: {venue_stats['friction_score'].quantile(0.90):.3f}\")\n",
    "    print(f\"  95th percentile: {venue_stats['friction_score'].quantile(0.95):.3f}\")\n",
    "    print(f\"  Max: {venue_stats['friction_score'].max():.3f}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 Venues with Highest Friction Scores:\")\n",
    "    top_venues = venue_stats.nlargest(10, 'friction_score')[\n",
    "        ['delay_rate', 'mean_delay_when_late', 'friction_score', 'order_count']\n",
    "    ]\n",
    "    top_venues['delay_rate_pct'] = (top_venues['delay_rate'] * 100).round(1)\n",
    "    print(top_venues[['delay_rate_pct', 'mean_delay_when_late', 'friction_score', 'order_count']].to_string())\n",
    "    \n",
    "    return venue_stats['friction_score'].to_dict()\n",
    "\n",
    "\n",
    "def apply_venue_friction_score(df: pd.DataFrame, friction_mapping: Dict[str, float]) -> pd.DataFrame:\n",
    "    \"\"\"Apply pre-calculated venue friction scores to a dataframe.\"\"\"\n",
    "    df['venue_friction_score'] = df['venue_location_h3_index'].map(friction_mapping).fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING: VENUE FRICTION SCORE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "venue_friction_mapping = calculate_venue_friction_score(train_df.copy())\n",
    "\n",
    "print(f\"\\nApplying venue friction scores...\")\n",
    "train_df = apply_venue_friction_score(train_df, venue_friction_mapping)\n",
    "print(f\"Training set: {len(train_df):,} rows\")\n",
    "print(f\"  - Known venues: {(train_df['venue_friction_score'] != 0).sum():,}\")\n",
    "print(f\"  - Unknown venues (score=0): {(train_df['venue_friction_score'] == 0).sum():,}\")\n",
    "\n",
    "val_df = apply_venue_friction_score(val_df, venue_friction_mapping)\n",
    "print(f\"Validation set: {len(val_df):,} rows\")\n",
    "print(f\"  - Known venues: {(val_df['venue_friction_score'] != 0).sum():,}\")\n",
    "print(f\"  - Unknown venues (score=0): {(val_df['venue_friction_score'] == 0).sum():,}\")\n",
    "\n",
    "test_df = apply_venue_friction_score(test_df, venue_friction_mapping)\n",
    "print(f\"Test set: {len(test_df):,} rows\")\n",
    "print(f\"  - Known venues: {(test_df['venue_friction_score'] != 0).sum():,}\")\n",
    "print(f\"  - Unknown venues (score=0): {(test_df['venue_friction_score'] == 0).sum():,}\")\n",
    "\n",
    "cleaned_df = apply_venue_friction_score(cleaned_df, venue_friction_mapping)\n",
    "print(f\"Full cleaned dataset: {len(cleaned_df):,} rows\")\n",
    "\n",
    "if 'delivery_error' in train_df.columns:\n",
    "    train_df = train_df.drop(columns=['delivery_error', 'is_delayed'])\n",
    "\n",
    "print(\"\\nVenue friction score feature engineering complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38544e38",
   "metadata": {},
   "source": [
    "### 14.5.4 Save Updated Datasets with New Features\n",
    "\n",
    "Re-save train/val/test splits with all engineered features included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c46e6",
   "metadata": {},
   "source": [
    "### 14.5.3 Distance × Weather Interaction Feature\n",
    "\n",
    "Create interaction feature between distance and precipitation based on EDA findings showing strong correlation improvement.\n",
    "\n",
    "**Rationale from EDA:**\n",
    "- Distance affects delivery time directly\n",
    "- Weather (precipitation) slows down couriers\n",
    "- The combined effect is non-additive: rain has a stronger impact on longer distances\n",
    "- This interaction captures how weather conditions amplify distance-based delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2f783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING: DISTANCE × WEATHER INTERACTION\n",
      "================================================================================\n",
      "\n",
      "Creating distance × weather interaction feature...\n",
      "Formula: distance_x_weather = h3_distance_km × precipitation\n",
      "\n",
      "✓ Applied to training set\n",
      "  - Non-zero interactions: 2,116 (12.7%)\n",
      "  - Mean: 0.097\n",
      "  - Median: 0.000\n",
      "  - Max: 9.600\n",
      "✓ Applied to validation set\n",
      "  - Non-zero interactions: 529 (14.8%)\n",
      "✓ Applied to test set\n",
      "  - Non-zero interactions: 273 (7.6%)\n",
      "✓ Applied to full cleaned dataset\n",
      "\n",
      "✓ Distance × weather interaction feature created!\n",
      "\n",
      "Interpretation:\n",
      "  • High values: Long distance + rainy weather → expect significant delays\n",
      "  • Zero values: Either no distance or no precipitation\n",
      "  • This feature helps model capture weather's amplified effect on long routes\n"
     ]
    }
   ],
   "source": [
    "def create_distance_weather_interaction(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create interaction feature between distance and weather.\"\"\"\n",
    "    df['distance_x_weather'] = df['h3_distance_km'] * df['precipitation']\n",
    "    return df\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING: DISTANCE × WEATHER INTERACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nCreating distance × weather interaction feature...\")\n",
    "print(\"Formula: distance_x_weather = h3_distance_km × precipitation\")\n",
    "\n",
    "train_df = create_distance_weather_interaction(train_df)\n",
    "print(f\"\\nApplied to training set\")\n",
    "print(f\"  - Non-zero interactions: {(train_df['distance_x_weather'] > 0).sum():,} ({(train_df['distance_x_weather'] > 0).sum()/len(train_df)*100:.1f}%)\")\n",
    "print(f\"  - Mean: {train_df['distance_x_weather'].mean():.3f}\")\n",
    "print(f\"  - Median: {train_df['distance_x_weather'].median():.3f}\")\n",
    "print(f\"  - Max: {train_df['distance_x_weather'].max():.3f}\")\n",
    "\n",
    "val_df = create_distance_weather_interaction(val_df)\n",
    "print(f\"Applied to validation set\")\n",
    "print(f\"  - Non-zero interactions: {(val_df['distance_x_weather'] > 0).sum():,} ({(val_df['distance_x_weather'] > 0).sum()/len(val_df)*100:.1f}%)\")\n",
    "\n",
    "test_df = create_distance_weather_interaction(test_df)\n",
    "print(f\"Applied to test set\")\n",
    "print(f\"  - Non-zero interactions: {(test_df['distance_x_weather'] > 0).sum():,} ({(test_df['distance_x_weather'] > 0).sum()/len(test_df)*100:.1f}%)\")\n",
    "\n",
    "cleaned_df = create_distance_weather_interaction(cleaned_df)\n",
    "print(f\"Applied to full cleaned dataset\")\n",
    "\n",
    "print(\"\\nDistance × weather interaction feature created!\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - High values: Long distance + rainy weather -> expect significant delays\")\n",
    "print(\"  - Zero values: Either no distance or no precipitation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb631383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING DATASETS WITH ENGINEERED FEATURES\n",
      "================================================================================\n",
      "✓ Saved: ../outputs/cleaned_full.csv\n",
      "  Shape: (23865, 16)\n",
      "  Columns: 16\n",
      "✓ Saved: ../outputs/train.csv\n",
      "  Shape: (16705, 16)\n",
      "✓ Saved: ../outputs/val.csv\n",
      "  Shape: (3580, 16)\n",
      "✓ Saved: ../outputs/test.csv\n",
      "  Shape: (3580, 16)\n",
      "\n",
      "✓ All datasets updated with new features!\n",
      "\n",
      "New columns added:\n",
      "  1. h3_grid_distance\n",
      "  2. h3_distance_km\n",
      "  3. venue_friction_score\n",
      "  4. distance_x_weather\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING DATASETS WITH ENGINEERED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_dir = '../outputs/'\n",
    "\n",
    "cleaned_path = output_dir + 'cleaned_full.csv'\n",
    "cleaned_df.to_csv(cleaned_path, index=False)\n",
    "print(f\"Saved: {cleaned_path}\")\n",
    "print(f\"  Shape: {cleaned_df.shape}\")\n",
    "print(f\"  Columns: {len(cleaned_df.columns)}\")\n",
    "\n",
    "train_path = output_dir + 'train.csv'\n",
    "train_df.to_csv(train_path, index=False)\n",
    "print(f\"Saved: {train_path}\")\n",
    "print(f\"  Shape: {train_df.shape}\")\n",
    "\n",
    "val_path = output_dir + 'val.csv'\n",
    "val_df.to_csv(val_path, index=False)\n",
    "print(f\"Saved: {val_path}\")\n",
    "print(f\"  Shape: {val_df.shape}\")\n",
    "\n",
    "test_path = output_dir + 'test.csv'\n",
    "test_df.to_csv(test_path, index=False)\n",
    "print(f\"Saved: {test_path}\")\n",
    "print(f\"  Shape: {test_df.shape}\")\n",
    "\n",
    "print(f\"\\nAll datasets updated with new features!\")\n",
    "print(f\"\\nNew columns added:\")\n",
    "print(f\"  1. h3_grid_distance\")\n",
    "print(f\"  2. h3_distance_km\")\n",
    "print(f\"  3. venue_friction_score\")\n",
    "print(f\"  4. distance_x_weather\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
